{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import interpolate\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import randint\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "import time\n",
    "import argparse\n",
    "from math import pi\n",
    "import torch.fft\n",
    "from torch.utils.data import Dataset\n",
    "from skimage.io import imread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_curr_dir = os.getcwd()\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "vx, vy, vz = 88e+03, 88e+03, 250e+03 # simulated volume in nm\n",
    "Nx, Ny, Nz= 512,512,50\n",
    "Nx_ext, Ny_ext, Nz_ext = 2*Nx, 2*Ny, Nz\n",
    "dx, dy, dz= vx/Nx, vy/Ny, vz/Nz\n",
    "dn=0.2\n",
    "wl = 650 #406 in nm\n",
    "NA = 0.9\n",
    "r = 500\n",
    "nb = 1.333\n",
    "k = 2*pi/wl*nb\n",
    "k0 = 2*pi/wl\n",
    "Rg=0.01\n",
    "k = 2*pi/wl*nb\n",
    "theta=0\n",
    "\n",
    "dkx_ext = 2*pi/(Nx_ext*dx)\n",
    "dky_ext = 2*pi/(Ny_ext*dy)\n",
    "kx_ext = dkx_ext * (np.array([k for k in range(Nx_ext//2)] + [ k for k in range (-Nx_ext//2,0)]))\n",
    "ky_ext = dky_ext * (np.array([k for k in range(Ny_ext//2)] + [ k for k in range (-Ny_ext//2,0)]))\n",
    "[Kyy_ext,Kxx_ext] = np.meshgrid(ky_ext**2, kx_ext**2)\n",
    "\n",
    "dphi_ext = np.real(k - np.sqrt((k**2 - Kxx_ext - Kyy_ext),dtype=\"complex64\")) #diffraction phase factor extended\n",
    "dphi_ext= torch.from_numpy(dphi_ext)\n",
    "\n",
    "pupil=np.zeros((Nx_ext,Ny_ext),dtype =\"complex64\")\n",
    "for a in range (Nx_ext):\n",
    "    for b in range(Ny_ext):\n",
    "        if np.sqrt(kx_ext[a]**2+ky_ext[b]**2)<= NA/wl:\n",
    "            pupil[a,b]=1\n",
    "        else: pupil[a,b]=0\n",
    "\n",
    "batch_size=1\n",
    "ntrain = 1\n",
    "nvalid = 1\n",
    "path_train = path_curr_dir + \"/Training/\"\n",
    "\n",
    "batch_size_gen = 1\n",
    "nvalid_batches = int(nvalid // batch_size_gen)\n",
    "ntrain_batches = int(ntrain / batch_size_gen)\n",
    "\n",
    "results_path = path_curr_dir + \"/Results/\"\n",
    "\n",
    "#Np= np.int(Rg*Nx*dx*Ny*dy/(np.pi*r**2))\n",
    "Np=1\n",
    "\n",
    "\n",
    "max_epochs = 50\n",
    "\n",
    "initial_learning_rate = 1\n",
    "\n",
    "scaling_factor = 3*dn\n",
    "\n",
    "ind_all = np.arange(0, ntrain_batches + nvalid_batches, 1)\n",
    "list_all = ind_all.tolist()\n",
    "list_IDs = [str(i) for i in list_all]\n",
    "\n",
    "train_IDs = list_IDs[:ntrain_batches]\n",
    "valid_IDs = list_IDs[ntrain_batches:]\n",
    "partition = {'train': train_IDs, 'valid': valid_IDs}\n",
    "\n",
    "def forwBPM(): #y en 3d pour pls éhantillons en même temps\n",
    "   \n",
    "    FPDist = -Nz*dz*0.5\n",
    "    #pupil=setup_params['pupil_function']\n",
    "    uin = torch.ones((Nx_ext,Ny_ext), dtype=torch.complex64) #à mettre sur gpu\n",
    "\n",
    "    y = uin\n",
    "\n",
    "    upad=torch.ones((Nx_ext,Ny_ext),dtype =torch.complex64)  #à mettre sur gpu\n",
    "    for ind_z in range (Nz):\n",
    "        y = torch.fft.ifftn(torch.fft.fftn(y,dim=(-2, -1))*torch.exp(-1j*dphi_ext*dz),dim=(-2, -1)) #Diffraction step\n",
    "        upad[(Nx_ext-Nx)//2:(Nx_ext+Nx)//2,(Ny_ext-Ny)//2:(Ny_ext+Ny)//2] = torch.exp(1j*k0*n_zxy[ind_z]*dz)\n",
    "\n",
    "        y=y*upad\n",
    "    yvide = y\n",
    "    y = torch.fft.ifftn(torch.fft.fftn(y,dim=(-2, -1))*torch.exp(-1j*dphi_ext*FPDist)*pupil,dim=(-2, -1))\n",
    "    yvide= torch.fft.ifftn(torch.fft.fftn(uin,dim=(-2, -1))*torch.exp(-1j*dphi_ext*(Nz*dz+FPDist)*pupil),dim=(-2, -1))\n",
    "    y=y-yvide\n",
    "    y = y[(Nx_ext-Nx)//2:(Nx_ext+Nx)//2,(Ny_ext-Ny)//2:(Ny_ext+Ny)//2] #torch.narrow à check\n",
    "\n",
    "    return y\n",
    "\n",
    "def save_checkpoint(state, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Sampling examples for training\n",
      "==================================================\n",
      "Training Example [1 / 1]\n",
      "==================================================\n",
      "Sampling examples for validation\n",
      "==================================================\n",
      "Validation Example [1 / 1]\n",
      "Finished sampling examples!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# random seed for repeatability\n",
    "torch.manual_seed(999)\n",
    "np.random.seed(566)\n",
    "\n",
    "# save the device to the settings dictionary\n",
    "\n",
    "if not (os.path.isdir(path_train)):\n",
    "    os.mkdir(path_train)\n",
    "    os.mkdir(path_train+\"/abs/\")\n",
    "    os.mkdir(path_train+\"/angle/\")\n",
    "    os.mkdir(path_train+\"/real/\")\n",
    "    os.mkdir(path_train+\"/imag/\")\n",
    "\n",
    "# print status\n",
    "print('=' * 50)\n",
    "print('Sampling examples for training')\n",
    "print('=' * 50)\n",
    "\n",
    "\n",
    "# generate examples for training\n",
    "labels_dict = {}\n",
    "\n",
    "\n",
    "for i in range(ntrain_batches):\n",
    "\n",
    "    # sample a training example\n",
    "    n_zxy = np.zeros((Nz,Nx,Ny),dtype=\"complex\")\n",
    "    i_obj=randint(5,Nz-6,Np),randint(5,Nx-6,Np),randint(5,Ny-6,Np)\n",
    "    z,x,y=np.array(i_obj)\n",
    "    z,x,y= int(z),int(x),int(y)\n",
    "    n_zxy[z-5:z+5,x-5:x+5,y-5:y+5] = dn\n",
    "    n_zxy=torch.from_numpy(n_zxy)\n",
    "    \n",
    "\n",
    "    y= forwBPM()\n",
    "\n",
    "    # save image as a tiff file and xyz to labels dict\n",
    "    im_name_tiff_abs = path_train+ \"/abs/\" + 'im_abs_' + str(i) + '.tiff'\n",
    "    img1_abs = Image.fromarray((np.abs(y.numpy())))\n",
    "    img1_abs.save(im_name_tiff_abs)\n",
    "\n",
    "    im_name_tiff_angle = path_train+ \"/angle/\" + 'im_angle_' + str(i) + '.tiff'\n",
    "    img1_angle = Image.fromarray((np.angle(y.numpy())))\n",
    "    img1_angle.save(im_name_tiff_angle)\n",
    "\n",
    "    im_name_tiff_imag = path_train+ \"/imag/\" + 'im_imag_' + str(i) + '.tiff'\n",
    "    img1_imag= Image.fromarray((np.imag(y.numpy())))\n",
    "    img1_imag.save(im_name_tiff_imag)\n",
    "\n",
    "    im_name_tiff_real = path_train+ \"/real/\" + 'im_real_' + str(i) + '.tiff'\n",
    "    img1_real = Image.fromarray((np.real(y.numpy())))\n",
    "    img1_real.save(im_name_tiff_real)\n",
    "\n",
    "    labels_dict[str(i)] = i_obj\n",
    "# print number of example\n",
    "    print('Training Example [%d / %d]' % (i + 1, ntrain_batches))\n",
    "\n",
    "# print status\n",
    "print('=' * 50)\n",
    "print('Sampling examples for validation')\n",
    "print('=' * 50)\n",
    "\n",
    "for i in range(nvalid_batches):\n",
    "\n",
    "    n_zxy = np.zeros((Nz,Nx,Ny),dtype=\"complex\")\n",
    "    i_obj=randint(5,Nz-6,Np),randint(5,Nx-6,Np),randint(5,Ny-6,Np)\n",
    "    z,x,y=np.array(i_obj)\n",
    "    z,x,y= int(z),int(x),int(y)\n",
    "    n_zxy[z-5:z+5,x-5:x+5,y-5:y+5] = dn\n",
    "    n_zxy=torch.from_numpy(n_zxy)\n",
    "\n",
    "\n",
    "    y= forwBPM()\n",
    "\n",
    "    # save image as a tiff file and xyz to labels dict\n",
    "    im_name_tiff_abs = path_train+ \"/abs/\" + 'im_abs_' + str(i+ntrain_batches) + '.tiff'\n",
    "    img1_abs = Image.fromarray((np.abs(y.numpy())))\n",
    "    img1_abs.save(im_name_tiff_abs)\n",
    "\n",
    "    im_name_tiff_angle = path_train+ \"/angle/\" + 'im_angle_' + str(i+ntrain_batches) + '.tiff'\n",
    "    img1_angle = Image.fromarray((np.angle(y.numpy())))\n",
    "    img1_angle.save(im_name_tiff_angle)\n",
    "\n",
    "    im_name_tiff_imag = path_train+ \"/imag/\" + 'im_imag_' + str(i+ntrain_batches) + '.tiff'\n",
    "    img1_imag= Image.fromarray((np.imag(y.numpy())))\n",
    "    img1_imag.save(im_name_tiff_imag)\n",
    "\n",
    "    im_name_tiff_real = path_train+ \"/real/\" + 'im_real_' + str(i+ntrain_batches) + '.tiff'\n",
    "    img1_real = Image.fromarray((np.real(y.numpy())))\n",
    "    img1_real.save(im_name_tiff_real)\n",
    "\n",
    "    labels_dict[str(i+ntrain_batches)] = i_obj\n",
    "    # print number of example\n",
    "    print('Validation Example [%d / %d]' % (i + 1, nvalid_batches))\n",
    "\n",
    "# save all xyz's dictionary as a pickle file\n",
    "path_labels = path_train + 'labels.pickle'\n",
    "with open(path_labels, 'wb') as handle:\n",
    "    pickle.dump(labels_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print('Finished sampling examples!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagesDataset(Dataset):\n",
    "    \n",
    "    # initialization of the dataset\n",
    "    def __init__(self, root_dir, list_IDs, labels):\n",
    "        self.root_dir = root_dir\n",
    "        self.list_IDs = list_IDs\n",
    "        self.labels = labels\n",
    "        \n",
    "        \n",
    "    # total number of samples in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.list_IDs)\n",
    "    \n",
    "    # sampling one example from the data\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # select sample\n",
    "        ID = self.list_IDs[index]\n",
    "        \n",
    "        # load tiff image\n",
    "        im_name_abs = self.root_dir + '/abs/'+ '/im_abs_' + ID + '.tiff'\n",
    "        im_np_abs = imread(im_name_abs)\n",
    "        \n",
    "        im_name_angle = self.root_dir + '/angle/'+ '/im_angle_' + ID + '.tiff'\n",
    "        im_np_angle = imread(im_name_angle)\n",
    "        \n",
    "        im_name_real = self.root_dir + '/real/'+ '/im_real_' + ID + '.tiff'\n",
    "        im_np_real = imread(im_name_real)\n",
    "        \n",
    "        im_name_imag = self.root_dir + '/imag/'+ '/im_imag_' + ID + '.tiff'\n",
    "        im_np_imag = imread(im_name_imag)\n",
    "        \n",
    "        im_np_abs = np.expand_dims(im_np_abs, 0)\n",
    "        im_np_angle = np.expand_dims(im_np_angle, 0)\n",
    "        im_np_real = np.expand_dims(im_np_real, 0)\n",
    "        im_np_imag = np.expand_dims(im_np_imag, 0)\n",
    "        \n",
    "        im_np=np.concatenate((im_np_abs,im_np_angle,im_np_real,im_np_imag),0)\n",
    "        \n",
    "        im_tensor=torch.from_numpy(im_np_abs)\n",
    "       \n",
    "        i_obj = self.labels[ID]\n",
    "        \n",
    "        n_zxy = np.zeros((Nz,Nx,Ny), dtype=\"float32\")\n",
    "        z,x,y=np.array(i_obj)\n",
    "        z,x,y= int(z),int(x),int(y)\n",
    "          \n",
    "        n_zxy[z-5:z+5,x-5:x+5,y-5:y+5] = dn\n",
    "      \n",
    "        n_zxy=torch.from_numpy(n_zxy)\n",
    "        \n",
    "        return im_tensor, n_zxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the basic Conv-LeakyReLU-BN\n",
    "class Conv2DLeakyReLUBN(nn.Module):\n",
    "    def __init__(self, input_channels, layer_width, kernel_size, padding, dilation, negative_slope):\n",
    "        super(Conv2DLeakyReLUBN, self).__init__()\n",
    "        self.conv = nn.Conv2d(input_channels, layer_width, kernel_size, 1, padding, dilation)\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope, inplace=True)\n",
    "        self.bn = nn.BatchNorm2d(layer_width,track_running_stats=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.lrelu(out)\n",
    "        out = self.bn(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Localization architecture\n",
    "class LocalizationCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LocalizationCNN, self).__init__()\n",
    "        self.norm = nn.BatchNorm2d(num_features=1, affine=True)\n",
    "        self.nc = 128\n",
    "        self.layer1 = Conv2DLeakyReLUBN(1, self.nc, 3, 1, 1, 0.2)\n",
    "        self.layer2 = Conv2DLeakyReLUBN(self.nc + 1, self.nc, 3, 1, 1, 0.2)\n",
    "        self.layer3 = Conv2DLeakyReLUBN(self.nc + 1, self.nc, 3, (2, 2), (2, 2), 0.2)\n",
    "        self.layer4 = Conv2DLeakyReLUBN(self.nc + 1, self.nc, 3, (4, 4), (4, 4), 0.2)\n",
    "        self.layer5 = Conv2DLeakyReLUBN(self.nc + 1, self.nc, 3, 1, 1, 0.2)\n",
    "        self.layer6 = Conv2DLeakyReLUBN(self.nc + 1, self.nc, 3, 1, 1, 0.2)\n",
    "        self.deconv1 = Conv2DLeakyReLUBN(self.nc + 1, self.nc, 3, 1, 1, 0.2)\n",
    "        self.deconv2 = Conv2DLeakyReLUBN(self.nc, self.nc, 3, 1, 1, 0.2)\n",
    "        self.layer7 = Conv2DLeakyReLUBN(self.nc, Nz, 3, 1, 1, 0.2)\n",
    "        self.layer8 = Conv2DLeakyReLUBN(Nz, Nz, 3, 1, 1, 0.2)\n",
    "        self.layer9 = Conv2DLeakyReLUBN(Nz, Nz, 3, 1, 1, 0.2)\n",
    "        self.layer10 = nn.Conv2d(Nz, Nz, kernel_size=1, dilation=1)\n",
    "        self.pred = nn.Hardtanh(min_val=0.0, max_val=scaling_factor)\n",
    "        #self.pred = nn.LeakyReLU()\n",
    "        \n",
    "    def forward(self, im):\n",
    "\n",
    "        # extract multi-scale features\n",
    "        im = self.norm(im)\n",
    "        out = self.layer1(im)\n",
    "        features = torch.cat((out, im), 1)\n",
    "        out = self.layer2(features) + out\n",
    "        features = torch.cat((out, im), 1)\n",
    "        out = self.layer3(features) + out\n",
    "        features = torch.cat((out, im), 1)\n",
    "        out = self.layer4(features) + out\n",
    "        features = torch.cat((out, im), 1)\n",
    "        out = self.layer5(features) + out\n",
    "        features = torch.cat((out, im), 1)\n",
    "        out = self.layer6(features) + out\n",
    "\n",
    "        # upsample by 4 in xy\n",
    "        features = torch.cat((out, im), 1)\n",
    "        out = interpolate(features, scale_factor=1)\n",
    "        out = self.deconv1(out)\n",
    "        out = interpolate(out, scale_factor=1)\n",
    "        out = self.deconv2(out)\n",
    "\n",
    "        # refine z and exact xy\n",
    "        out = self.layer7(out)\n",
    "        out = self.layer8(out) + out\n",
    "        out = self.layer9(out) + out\n",
    "\n",
    "        # 1x1 conv and leakyrelu for final result\n",
    "        out = self.layer10(out)\n",
    "        out = self.pred(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Epoch 1/50\n",
      "==================================================\n",
      "Mean training loss: 0.0871,\n",
      "lr= 1\n",
      "Mean validation loss: 0.1093\n",
      "Mean Validation Loss Improved from inf to 0.1093, Saving Model Weights...\n",
      "Epoch complete in 0h 0m 1s\n",
      "==================================================\n",
      "Epoch 2/50\n",
      "==================================================\n",
      "Mean training loss: 0.1086,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0318\n",
      "Mean Validation Loss Improved from 0.1093 to 0.0318, Saving Model Weights...\n",
      "Mean Training Validation Gap Is -0.0768, Saving Model Weights...\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 3/50\n",
      "==================================================\n",
      "Mean training loss: 0.0245,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0135\n",
      "Mean Validation Loss Improved from 0.0318 to 0.0135, Saving Model Weights...\n",
      "Mean Training Validation Gap Is -0.0109, Saving Model Weights...\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 4/50\n",
      "==================================================\n",
      "Mean training loss: 0.0087,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0094\n",
      "Mean Validation Loss Improved from 0.0135 to 0.0094, Saving Model Weights...\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 5/50\n",
      "==================================================\n",
      "Mean training loss: 0.0082,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0090\n",
      "Mean Validation Loss Improved from 0.0094 to 0.0090, Saving Model Weights...\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 6/50\n",
      "==================================================\n",
      "Mean training loss: 0.0079,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0086\n",
      "Mean Validation Loss Improved from 0.0090 to 0.0086, Saving Model Weights...\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 7/50\n",
      "==================================================\n",
      "Mean training loss: 0.0078,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0083\n",
      "Mean Validation Loss Improved from 0.0086 to 0.0083, Saving Model Weights...\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 8/50\n",
      "==================================================\n",
      "Mean training loss: 0.0077,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0081\n",
      "Mean Validation Loss Improved from 0.0083 to 0.0081, Saving Model Weights...\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 9/50\n",
      "==================================================\n",
      "Mean training loss: 0.0076,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0080\n",
      "No improvement in mean loss for 1 epochs\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 10/50\n",
      "==================================================\n",
      "Mean training loss: 0.0076,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0081\n",
      "No improvement in mean loss for 2 epochs\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 11/50\n",
      "==================================================\n",
      "Mean training loss: 0.0076,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0080\n",
      "No improvement in mean loss for 3 epochs\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 12/50\n",
      "==================================================\n",
      "Mean training loss: 0.0075,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0079\n",
      "Mean Validation Loss Improved from 0.0081 to 0.0079, Saving Model Weights...\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 13/50\n",
      "==================================================\n",
      "Mean training loss: 0.0075,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0079\n",
      "No improvement in mean loss for 1 epochs\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 14/50\n",
      "==================================================\n",
      "Mean training loss: 0.0075,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0078\n",
      "No improvement in mean loss for 2 epochs\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 15/50\n",
      "==================================================\n",
      "Mean training loss: 0.0075,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0078\n",
      "No improvement in mean loss for 3 epochs\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 16/50\n",
      "==================================================\n",
      "Mean training loss: 0.0075,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0078\n",
      "Mean Validation Loss Improved from 0.0079 to 0.0078, Saving Model Weights...\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 17/50\n",
      "==================================================\n",
      "Mean training loss: 0.0075,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0078\n",
      "No improvement in mean loss for 1 epochs\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 18/50\n",
      "==================================================\n",
      "Mean training loss: 0.0074,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0078\n",
      "No improvement in mean loss for 2 epochs\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 19/50\n",
      "==================================================\n",
      "Mean training loss: 0.0074,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0079\n",
      "No improvement in mean loss for 3 epochs\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 20/50\n",
      "==================================================\n",
      "Mean training loss: 0.0074,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0079\n",
      "No improvement in mean loss for 4 epochs\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 21/50\n",
      "==================================================\n",
      "Mean training loss: 0.0073,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0078\n",
      "No improvement in mean loss for 5 epochs\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 22/50\n",
      "==================================================\n",
      "Mean training loss: 0.0071,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0077\n",
      "Mean Validation Loss Improved from 0.0078 to 0.0077, Saving Model Weights...\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 23/50\n",
      "==================================================\n",
      "Mean training loss: 0.0067,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0072\n",
      "Mean Validation Loss Improved from 0.0077 to 0.0072, Saving Model Weights...\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 24/50\n",
      "==================================================\n",
      "Mean training loss: 0.0061,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0033\n",
      "Mean Validation Loss Improved from 0.0072 to 0.0033, Saving Model Weights...\n",
      "Mean Training Validation Gap Is -0.0028, Saving Model Weights...\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 25/50\n",
      "==================================================\n",
      "Mean training loss: 0.0043,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0014\n",
      "Mean Validation Loss Improved from 0.0033 to 0.0014, Saving Model Weights...\n",
      "Mean Training Validation Gap Is -0.0029, Saving Model Weights...\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 26/50\n",
      "==================================================\n",
      "Mean training loss: 0.0012,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0012\n",
      "Mean Validation Loss Improved from 0.0014 to 0.0012, Saving Model Weights...\n",
      "Mean Training Validation Gap Is -0.0000, Saving Model Weights...\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 27/50\n",
      "==================================================\n",
      "Mean training loss: 0.0003,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0011\n",
      "Mean Validation Loss Improved from 0.0012 to 0.0011, Saving Model Weights...\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 28/50\n",
      "==================================================\n",
      "Mean training loss: 0.0003,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0010\n",
      "Mean Validation Loss Improved from 0.0011 to 0.0010, Saving Model Weights...\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 29/50\n",
      "==================================================\n",
      "Mean training loss: 0.0003,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0009\n",
      "No improvement in mean loss for 1 epochs\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 30/50\n",
      "==================================================\n",
      "Mean training loss: 0.0002,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0009\n",
      "Mean Validation Loss Improved from 0.0010 to 0.0009, Saving Model Weights...\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 31/50\n",
      "==================================================\n",
      "Mean training loss: 0.0002,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0008\n",
      "No improvement in mean loss for 1 epochs\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 32/50\n",
      "==================================================\n",
      "Mean training loss: 0.0002,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0008\n",
      "No improvement in mean loss for 2 epochs\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 33/50\n",
      "==================================================\n",
      "Mean training loss: 0.0002,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0007\n",
      "Mean Validation Loss Improved from 0.0009 to 0.0007, Saving Model Weights...\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 34/50\n",
      "==================================================\n",
      "Mean training loss: 0.0002,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0007\n",
      "No improvement in mean loss for 1 epochs\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 35/50\n",
      "==================================================\n",
      "Mean training loss: 0.0002,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0007\n",
      "No improvement in mean loss for 2 epochs\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 36/50\n",
      "==================================================\n",
      "Mean training loss: 0.0002,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0006\n",
      "Mean Validation Loss Improved from 0.0007 to 0.0006, Saving Model Weights...\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 37/50\n",
      "==================================================\n",
      "Mean training loss: 0.0002,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0006\n",
      "No improvement in mean loss for 1 epochs\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 38/50\n",
      "==================================================\n",
      "Mean training loss: 0.0002,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0005\n",
      "No improvement in mean loss for 2 epochs\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 39/50\n",
      "==================================================\n",
      "Mean training loss: 0.0002,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0005\n",
      "No improvement in mean loss for 3 epochs\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 40/50\n",
      "==================================================\n",
      "Mean training loss: 0.0002,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0005\n",
      "Mean Validation Loss Improved from 0.0006 to 0.0005, Saving Model Weights...\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 41/50\n",
      "==================================================\n",
      "Mean training loss: 0.0002,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0005\n",
      "No improvement in mean loss for 1 epochs\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 42/50\n",
      "==================================================\n",
      "Mean training loss: 0.0002,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0004\n",
      "No improvement in mean loss for 2 epochs\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 43/50\n",
      "==================================================\n",
      "Mean training loss: 0.0002,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0004\n",
      "No improvement in mean loss for 3 epochs\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 44/50\n",
      "==================================================\n",
      "Mean training loss: 0.0002,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0004\n",
      "No improvement in mean loss for 4 epochs\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 45/50\n",
      "==================================================\n",
      "Mean training loss: 0.0001,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0004\n",
      "No improvement in mean loss for 5 epochs\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 46/50\n",
      "==================================================\n",
      "Mean training loss: 0.0001,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0004\n",
      "No improvement in mean loss for 6 epochs\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 47/50\n",
      "==================================================\n",
      "Mean training loss: 0.0001,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0004\n",
      "Mean Validation Loss Improved from 0.0005 to 0.0004, Saving Model Weights...\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 48/50\n",
      "==================================================\n",
      "Mean training loss: 0.0001,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0003\n",
      "No improvement in mean loss for 1 epochs\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 49/50\n",
      "==================================================\n",
      "Mean training loss: 0.0001,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0003\n",
      "Mean Validation Loss Improved from 0.0004 to 0.0003, Saving Model Weights...\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Epoch 50/50\n",
      "==================================================\n",
      "Mean training loss: 0.0001,\n",
      "lr= 1\n",
      "Mean validation loss: 0.0002\n",
      "No improvement in mean loss for 1 epochs\n",
      "Epoch complete in 0h 0m 0s\n",
      "==================================================\n",
      "Training complete in 0h 0m 21s\n",
      "Best Validation Loss: 0.000237\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# set random number generators for repeatability\n",
    "torch.manual_seed(999)\n",
    "np.random.seed(526)\n",
    "path_save= results_path\n",
    "if not(os.path.isdir(path_save)):\n",
    "    os.mkdir(path_save)\n",
    "    \n",
    "# open all locations pickle file\n",
    "path_pickle = path_train + 'labels.pickle'\n",
    "with open(path_pickle, 'rb') as handle:\n",
    "    labels = pickle.load(handle)\n",
    "\n",
    "# Parameters for data loaders\n",
    "params_train = {'batch_size': 1, 'shuffle': True}\n",
    "params_valid = {'batch_size': 1, 'shuffle': False}\n",
    "\n",
    "# instantiate the data class and create a datalaoder for training\n",
    "\n",
    "training_set = ImagesDataset(path_train, partition['train'], labels)\n",
    "training_generator = DataLoader(training_set, **params_train)\n",
    "\n",
    "# instantiate the data class and create a datalaoder for validation\n",
    "validation_set = ImagesDataset(path_train, partition['valid'], labels)\n",
    "validation_generator = training_generator\n",
    "#validation_generator = DataLoader(validation_set, **params_valid)\n",
    "\n",
    "# build model and convert all the weight tensors to cuda()\n",
    "\n",
    "cnn = LocalizationCNN()\n",
    "cnn.to(device)\n",
    "\n",
    "# gap between validation and training loss\n",
    "gap_thresh = 1e-4\n",
    "steps_per_epoch = ntrain_batches/batch_size\n",
    "\n",
    "# adam optimizer\n",
    "optimizer = Adam(list(cnn.parameters()), lr=initial_learning_rate)\n",
    "\n",
    "# learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True, min_lr=1e-6)\n",
    "\n",
    "# loss function\n",
    "\n",
    "#criterion = KDE_loss3D(scaling_factor) \n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# start from scratch\n",
    "start_epoch, end_epoch = 0, max_epochs\n",
    "\n",
    "# initialize the learning results dictionary\n",
    "learning_results = {'train_loss': [], 'valid_loss': [],\n",
    "                    'max_valid': [], 'sum_valid': [], 'steps_per_epoch': steps_per_epoch}\n",
    "\n",
    "# initialize validation set loss to be infinity\n",
    "valid_loss_prev = float('Inf')\n",
    "\n",
    "# starting time of training\n",
    "train_start = time.time()\n",
    "\n",
    "# loop over epochs\n",
    "not_improve = 0\n",
    "for epoch in np.arange(start_epoch, end_epoch):\n",
    "\n",
    "    # starting time of current epoch\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    # print current epoch number\n",
    "    print('='*50)\n",
    "    print('Epoch {}/{}'.format(epoch+1, end_epoch))\n",
    "    print('='*50)\n",
    "\n",
    "    # training phase\n",
    "    cnn.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    with torch.set_grad_enabled(True):\n",
    "        for batch_ind, (inputs, targets) in enumerate(training_generator):\n",
    "\n",
    "            # transfer data to variable on GPU\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "#tau = range de criterion \n",
    "            # forward + backward + optimize\n",
    "            optimizer.zero_grad()\n",
    "            outputs = cnn(inputs)\n",
    "            outputs = outputs.to(device)\n",
    "            #print(np.unique(outputs.cpu().detach().numpy(), return_counts=True ))\n",
    "            #print(np.unique(targets.cpu().detach().numpy(), return_counts=True))\n",
    "            loss = criterion(outputs, targets) #+ tau*torch.nn.L1loss()(outputs, torch.zeros_like(outputs))\n",
    "            #print('train loss', loss)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # running statistics\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # print training loss\n",
    "            #print('Epoch [%d/%d], Iter [%d/%d], Loss: %.4f\\n' % (epoch+1,\n",
    "                  #end_epoch, batch_ind+1, steps_per_epoch, loss.item()))\n",
    "\n",
    "    # calculate and print mean validation loss \n",
    "    mean_train_loss = train_loss*batch_size/ntrain_batches\n",
    "\n",
    "    print('Mean training loss: %.4f,'\n",
    "          %(mean_train_loss))\n",
    "\n",
    "    # record training loss and \n",
    "    learning_results['train_loss'].append(mean_train_loss)\n",
    "    print(\"lr=\",optimizer.param_groups[0]['lr'])\n",
    "    # validation phase\n",
    "    cnn.eval()\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for batch_ind, (inputs, targets) in enumerate(validation_generator):\n",
    "\n",
    "            # transfer data to GPU\n",
    "            inputs = inputs.to(device)\n",
    "            #print(np.unique(inputs.cpu(), return_counts=True))\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # forward\n",
    "            optimizer.zero_grad()\n",
    "            outputs = cnn(inputs)\n",
    "            #print(np.unique(outputs.cpu().detach().numpy(), return_counts=True ))\n",
    "            #print(np.unique(targets.cpu().detach().numpy(), return_counts=True))\n",
    "            val_loss = criterion(outputs, targets)\n",
    "            #print('valid loss', val_loss)\n",
    "            # running statistics\n",
    "            valid_loss += val_loss.item()\n",
    "\n",
    "\n",
    "    # calculate and print mean validation loss \n",
    "    mean_valid_loss = valid_loss*batch_size/nvalid_batches\n",
    "\n",
    "    print('Mean validation loss: %.4f'%mean_valid_loss)\n",
    "\n",
    "    # record validation loss \n",
    "    learning_results['valid_loss'].append(mean_valid_loss)\n",
    "\n",
    "    # reduce learning rate if loss stagnates\n",
    "    scheduler.step(mean_valid_loss)\n",
    "\n",
    "\n",
    "    # saving checkpoint: save best model so far\n",
    "    if mean_valid_loss < (valid_loss_prev - 1e-4):\n",
    "\n",
    "        # print an update and save the model weights\n",
    "        print('Mean Validation Loss Improved from %.4f to %.4f, Saving Model Weights...'\n",
    "              % (valid_loss_prev, mean_valid_loss))\n",
    "        torch.save(cnn.state_dict(), path_save + 'weights_best_loss.pkl')\n",
    "\n",
    "        # change minimal loss and init stagnation indicator\n",
    "        valid_loss_prev = mean_valid_loss\n",
    "        not_improve = 0\n",
    "\n",
    "        # save model and current loss + optimizer\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': cnn.state_dict(),\n",
    "            'best_loss': mean_valid_loss,\n",
    "            'optimizer': optimizer.state_dict()}, path_save + 'checkpoint.pth.tar')\n",
    "    else:\n",
    "        # update stagnation indicator\n",
    "        not_improve += 1\n",
    "        print('No improvement in mean loss for %d epochs' % not_improve)\n",
    "\n",
    "    # save also when training departs from validation\n",
    "    train_valid_gap = mean_valid_loss - mean_train_loss\n",
    "    if train_valid_gap < gap_thresh:\n",
    "\n",
    "        # print an update and save the model weights\n",
    "        print('Mean Training Validation Gap Is %.4f, Saving Model Weights...' % train_valid_gap)\n",
    "        torch.save(cnn.state_dict(), path_save + 'weights_best_gap.pkl')\n",
    "\n",
    " \n",
    "    # report time it takes the net to complete an epoch\n",
    "    epoch_time_elapsed = time.time() - epoch_start_time\n",
    "    print('Epoch complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n",
    "            epoch_time_elapsed // 3600, \n",
    "            np.floor((epoch_time_elapsed / 3600 - epoch_time_elapsed // 3600)*60), \n",
    "            epoch_time_elapsed % 60))\n",
    "\n",
    "    # save all records for latter visualization\n",
    "    path_learning_results = path_save + 'learning_results.pickle'\n",
    "    with open(path_learning_results, 'wb') as handle:\n",
    "        pickle.dump(learning_results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # if no improvement for more than 50 epochs break training\n",
    "    if not_improve >= 10:\n",
    "        break\n",
    "\n",
    "# measure time that took the model to train\n",
    "train_time_elapsed = time.time() - train_start\n",
    "print('='*50)\n",
    "print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n",
    "        train_time_elapsed // 3600, \n",
    "        np.floor((train_time_elapsed/3600 - train_time_elapsed // 3600)*60), \n",
    "        train_time_elapsed % 60))\n",
    "\n",
    "# print a summary of best test loss\n",
    "print('Best Validation Loss: {:4f}'.format(mean_valid_loss))\n",
    "\n",
    "# save training time and best loss \n",
    "learning_results['epoch_converged'] = epoch\n",
    "learning_results['last_epoch_time'], learning_results['training_time'] = epoch_time_elapsed, train_time_elapsed\n",
    "\n",
    "path_learning_results = path_save + 'learning_results.pickle'\n",
    "with open(path_learning_results, 'wb') as handle:\n",
    "    pickle.dump(learning_results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    \n",
    "plt.figure(1)\n",
    "# x axis for the plot\n",
    "steps_per_epoch = learning_results['steps_per_epoch']\n",
    "iter_axis = np.arange(steps_per_epoch, steps_per_epoch * (epoch + 1) + 1, steps_per_epoch)        \n",
    "\n",
    "# plot result\n",
    "plt.clf()\n",
    "plt.figure(figsize=(10,10))\n",
    "linet, linev = plt.plot(iter_axis, learning_results['train_loss'], '-og', iter_axis, learning_results['valid_loss'], '-^r')\n",
    "plt.ylabel(\"Mean Loss\")\n",
    "plt.legend((linet, linev), ('Train', 'Valid'))\n",
    "plt.title(\"Training Metrics at Epoch %d\" % (epoch + 1))\n",
    "plt.draw()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cnn.load_state_dict(torch.load(path_results + 'weights_best_loss.pkl'))\n",
    "cnn.eval()\n",
    "\n",
    "stock_outputs=torch.empty((nvalid,Nz,512,512), device=\"cuda:3\")\n",
    "stock_targets=torch.empty((nvalid,Nz,512,512), device=\"cuda:3\")\n",
    "stock_inputs=torch.empty((nvalid,4,512,512), device=\"cuda:3\")\n",
    "\n",
    "with torch.set_grad_enabled(False):\n",
    "    for batch_ind, (inputs, targets) in enumerate(validation_generator):\n",
    "\n",
    "        # transfer data to GPU\n",
    "        stock_inputs[batch_ind] = inputs\n",
    "        inputs = inputs.to(device)\n",
    "        #print(np.unique(inputs.cpu(), return_counts=True))\n",
    "       \n",
    "        targets = targets.to(device)\n",
    "        #print(np.unique(targets.cpu(), return_counts=True))\n",
    "      \n",
    "        stock_targets[batch_ind] = targets\n",
    "        outputs = cnn(inputs)\n",
    "        stock_outputs[batch_ind] = outputs\n",
    "        \n",
    "inputs = stock_inputs[0]\n",
    "outputs = stock_outputs[0]\n",
    "targets = stock_targets[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8,5))\n",
    "#plt.subplots_adjust(left=0.25, bottom=0.25)\n",
    "fig.add_subplot(1,2,1,xticks=[], yticks=[])              \n",
    "plt.imshow(inputs.cpu()[0,:,:], cmap='gray',aspect='auto')\n",
    "plt.title(\"inputs\")\n",
    "fig.add_subplot(1,2,2,xticks=[], yticks=[])\n",
    "plt.imshow( np.abs(pupil, cmap='gray', aspect='auto') \n",
    "plt.title(\"pupil\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, sharey=True, figsize=(10,4))\n",
    "plt.subplots_adjust(left=0.25, bottom=0.25)\n",
    "fig.add_subplot(1,2,1,xticks=[], yticks=[])\n",
    "\n",
    "l1 = plt.imshow(outputs.cpu()[0,:,:], cmap='gray')\n",
    "plt.title(\"outputs\")\n",
    "plt.colorbar()     \n",
    "\n",
    "fig.add_subplot(1,2,2,xticks=[], yticks=[])\n",
    "plt.clim(0,0.2)\n",
    "l2 = plt.imshow(targets.cpu()[0,:,:], cmap='gray') \n",
    "plt.title(\"targets\")\n",
    "plt.colorbar()   \n",
    "plt.clim(0,0.2)\n",
    "axcolor = 'lightgoldenrodyellow'\n",
    "axh = plt.axes([0, 0.1, 0.65, 0.03], facecolor=axcolor)\n",
    "sh=Slider(axh,'z',1,Nz, valinit=1,valstep=1)\n",
    "\n",
    "def update(new_z):\n",
    "    new_z=sh.val\n",
    "    \n",
    "    l1.set_data(outputs.cpu()[new_z])\n",
    "    l2.set_data(targets.cpu()[new_z])\n",
    "\n",
    "    fig.canvas.draw_idle()\n",
    "sh.on_changed(update)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
